{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biol 359A  | Descriptive statistics and comparing groups\n",
    "### Spring 2022, Week 2\n",
    "<hr>\n",
    "\n",
    "Objectives:\n",
    "-  Read basic python syntax\n",
    "-  Run and interpret a t-test\n",
    "-  Gain intuition about statistical tests and sample sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of bash commands to make google colab work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/BIOL359A-FoundationsOfQBio-Spr22/week2_statisticaltests\n",
    "!mkdir ./data\n",
    "!cp week2_statisticaltests/data/* ./data\n",
    "!cp week2_statisticaltests/clean_data.py ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements\n",
    "\n",
    "Import statements are used to integrate *external code or packages* into our analysis. \n",
    "\n",
    "- `pandas`: Represents data as tables\n",
    "- `seaborn`: Data exploration visualization tool\n",
    "- `ipywidgets`: Notebook widgets that add user interfaces to notebooks\n",
    "- `random`: Generate random numbers\n",
    "- `numpy`: General math/matrices package\n",
    "- `matplotlib`: Data visualization software\n",
    "- `Scipy`: General scientific computing\n",
    "\n",
    "Using `as` will alias the package in the code.\n",
    "`matplotlib.pyplot` is importing the submodule `pyplot` from `matplotlib`. \n",
    "`from scipy.stats` is telling python where to find `ttest_ind`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind as ttest\n",
    "\n",
    "TITLE_FONT = 20\n",
    "LABEL_FONT = 16\n",
    "TICK_FONT = 16\n",
    "FIG_SIZE = (15,15)\n",
    "COLORS= [\"#008080\",\"#CA562C\"]\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common data visualizations\n",
    "\n",
    "First we are going to create a toy dataset using a random number generator. \n",
    "Here we use the Normal distribution function (pdf):\n",
    "\n",
    "\n",
    "$$\n",
    "  f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \n",
    "  \\exp\\left( -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{\\!2}\\,\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normal_distribution(n = 100, mu = 6, sigma = 2):\n",
    "    return [random.gauss(mu, sigma) for _ in range(0,n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_a_histogram(x, n, mu, sigma):\n",
    "    \"\"\"Generate annotated histogram based on normal (gaussian) distribution\"\"\"\n",
    "    data_color = COLORS[0]\n",
    "    annotation_color= COLORS[1]\n",
    "    plt.figure(figsize=FIG_SIZE)\n",
    "    sns.histplot(x, color=data_color, kde=True, stat=\"probability\")\n",
    "    _, xmax, _, ymax = plt.axis()\n",
    "    plt.title(\"Histogram of random values generated from a normal distribution\", fontsize=TITLE_FONT)\n",
    "    \n",
    "    plt.axvline(mu, linestyle='--', color=annotation_color, lw=3)\n",
    "    plt.text(mu, .97*ymax, r' $\\mu$', color=annotation_color, fontsize=LABEL_FONT, ma=\"left\")\n",
    "    \n",
    "    plt.axvline(mu+sigma, linestyle=':',color=annotation_color, lw=3)\n",
    "    plt.text(mu+sigma, .97*ymax, r' $\\mu+\\sigma$', color=annotation_color, fontsize=LABEL_FONT, ma=\"left\")\n",
    "\n",
    "    plt.axvline(mu-sigma, linestyle=':',color=annotation_color, lw=3)\n",
    "    plt.text(mu-sigma, .97*ymax, r' $\\mu-\\sigma$', color=annotation_color, fontsize=LABEL_FONT, ma=\"right\")\n",
    "    \n",
    "    plt.text(.8*xmax, .9*ymax, 'Data', color=data_color, fontsize=LABEL_FONT, weight=\"bold\")\n",
    "    plt.text(.8*xmax, .93*ymax, 'Underlying distribution', color=annotation_color, fontsize=LABEL_FONT, weight=\"bold\")\n",
    "    \n",
    "    sns.despine()\n",
    "\n",
    "    plt.show()\n",
    " \n",
    "@widgets.interact_manual(n=(3,1000), mu=(-10, 10), sigma=(0,10))\n",
    "def create_histplot(n=100, mu=6, sigma=2): \n",
    "    \"\"\"Wrapper function for widgets decorator and plot_a_histogram()\"\"\"\n",
    "    toy_dataset_x = generate_normal_distribution(n=n, mu=mu, sigma=sigma)\n",
    "    plot_a_histogram(toy_dataset_x, n, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Histograms__ use discrete bins (range of values) to categorize data, and are used primarily to visualize probability or proportions. Most visualizations of probability are based on histogram-like structures, and the kernel density estimate (KDE) line is the best guess at an underlying distribution. The higher the bar in a bin, the more times a value occurs in that bin within a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_a_boxplot(x, n, mu, sigma, swarm):\n",
    "    \"\"\"Generate annotated boxplot based on normal (gaussian) distribution\"\"\"\n",
    "    data_color = COLORS[0]\n",
    "    annotation_color= COLORS[1]\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.boxplot(data=x, color=data_color,orient='h', boxprops=dict(alpha=.5), showmeans=True,\n",
    "                meanprops={\"marker\":\"o\",\n",
    "                           \"markerfacecolor\":\"white\", \n",
    "                           \"markeredgecolor\":\"black\",\n",
    "                           \"markersize\":\"6\"})\n",
    "    if swarm: sns.stripplot(data=x, orient='h',size=5, color=\".3\", linewidth=0)\n",
    "    \n",
    "    _, xmax, _, ymax = plt.axis()\n",
    "    plt.title(\"Boxplot of random values generated from a normal distribution\", fontsize=TITLE_FONT)\n",
    "    \n",
    "    plt.axvline(mu, linestyle='--', color=annotation_color, lw=3)\n",
    "    plt.text(mu, -0.9*ymax, r' $\\mu$', color=annotation_color, fontsize=LABEL_FONT, ma=\"left\")\n",
    "    \n",
    "    plt.axvline(mu+sigma, linestyle=':',color=annotation_color, lw=3)\n",
    "    plt.text(mu+sigma, -0.9*ymax, r' $\\mu+\\sigma$', color=annotation_color, fontsize=LABEL_FONT, ma=\"left\")\n",
    "\n",
    "    plt.axvline(mu-sigma, linestyle=':',color=annotation_color, lw=3)\n",
    "    plt.text(mu-sigma, -0.9*ymax, r' $\\mu-\\sigma$', color=annotation_color, fontsize=LABEL_FONT, ma=\"right\")\n",
    "    \n",
    "    plt.text(0.75*xmax, 0.6*ymax, 'Data', color=data_color, fontsize=LABEL_FONT, weight=\"bold\")\n",
    "    plt.text(0.75*xmax, 0.8*ymax, 'Underlying distribution', color=annotation_color, fontsize=LABEL_FONT, weight=\"bold\")\n",
    "    sns.despine()\n",
    "    plt.show()    \n",
    "    \n",
    "    \n",
    "@widgets.interact_manual(n=(3,1000), mu=(-10, 10), sigma=(0,10))\n",
    "def create_boxplot(n=100, mu=6, sigma=2, swarm=True): \n",
    "    \"\"\"Wrapper function for widgets decorator and plot_a_boxplot()\"\"\"\n",
    "    toy_dataset_x = generate_normal_distribution(n=n, mu=mu, sigma=sigma)\n",
    "    plot_a_boxplot(toy_dataset_x, n, mu, sigma, swarm=swarm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Box plots__ are common for comparing data, which we will be using later. The _box_ represents the interquartile range, or the data that contains the middle 50 percent of the data. The _whiskers_ represent the range of the distribution, adjusted for outliers, represented by _diamonds_. The _circle_ represents the sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_a_scatterplot(x, y, n, mu_x, sigma_x, mu_y, sigma_y):\n",
    "    \"\"\"Generate annotated boxplot based on normal (gaussian) distribution\"\"\"\n",
    "    data_color = COLORS[0]\n",
    "    annotation_color= COLORS[1]\n",
    "    sns.jointplot(x=x, \n",
    "                  y=y,\n",
    "                  kind=\"reg\",color=data_color,\n",
    "                  marginal_kws=dict(bins=10,color=annotation_color));\n",
    "    plt.xlabel(r\"X data $\\mu={0}$,$\\sigma={1}$\".format(mu_x,sigma_x))\n",
    "    plt.ylabel(r\"Y data $\\mu={0}$,$\\sigma={1}$\".format(mu_y,sigma_y))\n",
    "\n",
    "    sns.despine()\n",
    "    plt.show()  \n",
    "    \n",
    "@widgets.interact_manual(n=(3,1000), mu_x=(-10, 10), sigma_x=(0,10), mu_y=(-10, 10), sigma_y=(0,10))\n",
    "def create_scatterplot(n=100, mu_x=6, sigma_x=2, mu_y=6, sigma_y=2): \n",
    "    \"\"\"Wrapper function for widgets decorator and plot_a_scatterplot()\"\"\"\n",
    "    toy_dataset_x = generate_normal_distribution(n=n, mu=mu_x, sigma=sigma_x)\n",
    "    toy_dataset_y = generate_normal_distribution(n=n, mu=mu_y, sigma=sigma_y)\n",
    "    plot_a_scatterplot(toy_dataset_x, toy_dataset_y, n, mu_x, sigma_x, mu_y, sigma_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Scatter plots__ are designed to show *pairs* of data points. Each X-axis data point is associated with a Y-axis data point. These plots will be common when we're talking about associations between continuous variables, specifically for discussions of correlation (and regression - you can ignore the line through the points for now). The histograms have been included to help associate the data sets to the scatter points. Remember, these data were randomly generated and paired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition: Central limit theorem\n",
    "\n",
    "You can ignore the code below! It is setting up an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_datasets(n, rng, repetitions=200):\n",
    "    \"\"\"Generate (repititions) of sample averages, based on (n) samples per average\"\"\"\n",
    "    averages = []\n",
    "    all_nums = []\n",
    "    for i in range(0, repetitions):\n",
    "        nums = [generate_random_numbers(rng) for _ in range(0,n)]\n",
    "        all_nums += nums\n",
    "        averages.append(np.mean(nums))\n",
    "    return all_nums, averages\n",
    "    \n",
    "def generate_histograms_clt(axs, n=10, rng=\"uniform\"):\n",
    "    \"\"\"build 2x2 matrix of histograms\"\"\"\n",
    "\n",
    "    all_nums_fixed, averages_fixed = get_random_datasets(10, rng)\n",
    "    build_paired_histograms(averages_fixed, all_nums_fixed, 10, rng, axs, column=0)\n",
    "    \n",
    "    all_nums, averages = get_random_datasets(n, rng)\n",
    "    build_paired_histograms(averages, all_nums, n, rng, axs, column=1)\n",
    "    \n",
    "def build_paired_histograms(averages, all_nums, n, rng, axs, column):\n",
    "    \"\"\"histogram plotting specific to this lecture\"\"\"\n",
    "    colors=[\"#1e81b0\", \"#e28743\"]\n",
    "    color = colors[column]\n",
    "    axs[0,column].set_title(f\"random samples\")\n",
    "    axs[1,column].set_title(f\"sample averages\")\n",
    "    axs[0,column].text(0.9, 0.9, f\"n:{n}\"+r\" $\\mu$: {0:.2f}, $\\sigma$:{1:.2f}\".format(np.mean(all_nums), np.std(all_nums)) ,\n",
    "                       verticalalignment='bottom', horizontalalignment='right',\n",
    "                       transform=axs[0,column].transAxes,\n",
    "                       color=color, fontsize=LABEL_FONT)\n",
    "    axs[1,column].text(0.9, 0.9, f\"n:{n}\"+r\" $\\mu$: {0:.2f}, $\\sigma$:{1:.2f}\".format(np.mean(averages), np.std(averages)),\n",
    "                       verticalalignment='bottom', horizontalalignment='right',\n",
    "                       transform=axs[1,column].transAxes,\n",
    "                       color=color, fontsize=LABEL_FONT)\n",
    "    sns.histplot(all_nums, ax=axs[0, column], color = color, stat=\"probability\", kde=True)\n",
    "    sns.histplot(averages, bins=10, ax=axs[1, column], color = color, stat=\"probability\", kde=True)\n",
    "    axs[1, column].set_xlim(0,10)\n",
    "    \n",
    "    \n",
    "def generate_random_numbers(generator = \"uniform\"):\n",
    "    \"\"\"generate random numbers with a mean of 5\"\"\"\n",
    "    if generator == \"uniform\": return random.uniform(0,10)\n",
    "    elif generator == \"exponential\": return random.expovariate(1/5)\n",
    "    elif generator == \"normal\": return random.gauss(5,2)\n",
    "    \n",
    "def format_plots(axs):\n",
    "    \"\"\"some extra formatting for subplots\"\"\"\n",
    "    for ax in axs.flat:\n",
    "        title = ax.get_title()\n",
    "        ax.set_title(title, fontweight=\"bold\", size=LABEL_FONT)\n",
    "        ax.set_ylabel('Proportion (Probability)', fontsize = LABEL_FONT)\n",
    "        ax.set_xlabel('Number', fontsize = LABEL_FONT)\n",
    "        ax.tick_params(labelsize=TICK_FONT)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The central limit theorem \n",
    "\n",
    "\n",
    "There are a number of different ways that the CLT can be proven, but the CLT states the following:\n",
    "\n",
    "$$\\bar{X} \\xrightarrow[]{d} N\\Big(\\mu, \\frac{\\sigma^2}{n}\\Big)$$\n",
    "\n",
    "This means that with repeated sampling, $\\bar{X}$ will converge to a normal distribution, centered at $\\mu$ or the mean of the original distribution, with a standard deviation of $\\sigma/\\sqrt{n}$ or the standard deviation of our original distribution divided by the number of sample averages in the distribution. We will show by using the following procedure:\n",
    "\n",
    "Here we a couple of plots for you to interact with. The left-side plots are fixed to collecting 10 samples from the distribution. The top row of plots shows the histograms of the underlying data points (here n samples x 200 repititions) and the bottom row shows the distribution of averages (200 repititions). Are the results what you would expect from the central limit theorem (CLT)? Does it hold true for other distributions? Is there a distribution shown here that the CLT does not hold true for? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact_manual(n=(3,100),  generator=[\"uniform\",\"exponential\",\"normal\"])\n",
    "def demonstrate_clt(n=10, generator=\"exponential\"):\n",
    "    \"\"\"wrapper function for CLT and widgets decorator\"\"\"\n",
    "    random.seed(10)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=FIG_SIZE, constrained_layout=True)\n",
    "    fig.suptitle(f\"Random number generator (distribution): {generator}\",fontweight=\"bold\", size=TITLE_FONT)\n",
    "    generate_histograms_clt(axs, n, generator)\n",
    "    format_plots(axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding our data\n",
    "\n",
    "We are going to import a dataset using another python script. The python script is loading the file and doing some basic cleaning of parts of the dataset we aren't using. It can be found in `clean_data.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clean_data #helper function with \n",
    "\n",
    "cancer_dataset = clean_data.generate_clean_dataframe()\n",
    "cancer_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have a basic understanding of the structure of the data now. \n",
    "\n",
    "From the data source: Wisconsin Diagnostic Breast Cancer (WDBC)\n",
    "\n",
    "```\n",
    "\tFeatures are computed from a digitized image of a fine needle\n",
    "\taspirate (FNA) of a breast mass.  They describe\n",
    "\tcharacteristics of the cell nuclei present in the image.\n",
    "\tA few of the images can be found at\n",
    "\thttp://www.cs.wisc.edu/~street/images/\n",
    "\n",
    "\tSeparating plane described above was obtained using\n",
    "\tMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
    "\tConstruction Via Linear Programming.\" Proceedings of the 4th\n",
    "\tMidwest Artificial Intelligence and Cognitive Science Society,\n",
    "\tpp. 97-101, 1992], a classification method which uses linear\n",
    "\tprogramming to construct a decision tree.  Relevant features\n",
    "\twere selected using an exhaustive search in the space of 1-4\n",
    "\tfeatures and 1-3 separating planes.\n",
    "\n",
    "\tThe actual linear program used to obtain the separating plane\n",
    "\tin the 3-dimensional space is that described in:\n",
    "\t[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
    "\tProgramming Discrimination of Two Linearly Inseparable Sets\",\n",
    "\tOptimization Methods and Software 1, 1992, 23-34].\n",
    "    \n",
    "    Source:\n",
    "    W.N. Street, W.H. Wolberg and O.L. Mangasarian \n",
    "\tNuclear feature extraction for breast tumor diagnosis.\n",
    "\tIS&T/SPIE 1993 International Symposium on Electronic Imaging: Science\n",
    "\tand Technology, volume 1905, pages 861-870, San Jose, CA, 1993.\n",
    "```\n",
    "\n",
    "What do all the column names mean?\n",
    "\n",
    "- ID number\n",
    "- Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "- radius (mean of distances from center to points on the perimeter)\n",
    "- texture (standard deviation of gray-scale values)\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness (local variation in radius lengths)\n",
    "- compactness (perimeter^2 / area - 1.0)\n",
    "- concavity (severity of concave portions of the contour)\n",
    "- concave points (number of concave portions of the contour)\n",
    "- symmetry \n",
    "- fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "\n",
    "Cateogory Distribution: 357 benign, 212 malignant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to show the first five values in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset[\"mean_area\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to show the first five values from the groups in the diagnosis column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset[\"mean_area\"].groupby(\"diagnosis\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to calculate basic descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_count(x):\n",
    "    \"\"\"calculate how many values are in a dataset\"\"\"\n",
    "    return len(x)\n",
    "\n",
    "def calculate_mean(x):\n",
    "    \"\"\"get the sample mean\"\"\"\n",
    "    return np.sum(x) / len(x)\n",
    "\n",
    "def calculate_variance(x):\n",
    "    \"\"\"calculate variance of the dataset\"\"\"\n",
    "    return calculate_mean((x - calculate_mean(x))**2)\n",
    "\n",
    "def calculate_std(x):\n",
    "    \"\"\"calculate standard deviation from the square of the variance\"\"\"\n",
    "    return np.sqrt(calculate_variance(x))\n",
    "\n",
    "area = cancer_dataset[\"mean_area\"]\n",
    "\n",
    "print(f\"count =  {calculate_count(area):.0f}\")\n",
    "print(f\"mean =  {calculate_mean(area):.2f}\")\n",
    "print(f\"var =  {calculate_variance(area):.2f}\")\n",
    "print(f\"std =  {calculate_std(area):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` has us covered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset[\"mean_area\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice a difference? The standard deviation is being calculated differently. Simply taking the square root of the variance doesn't give you an unbiased estimator. What is the main difference between the estimate in `calculate_std_est()` and the previous one in `calculate_std()`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unbiased_std(x):\n",
    "    \"\"\"unbiased estimator by __________\"\"\"\n",
    "    return np.sqrt(np.sum((x - calculate_mean(x))**2)/(len(x)-1))\n",
    "\n",
    "print(f\"unbiased std =  {calculate_std_est(area):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing variables\n",
    "\n",
    "What if we're interested in the relationship between variables? Here we calculate the Pearson correlation coefficient $\\rho$. Which variables appear correlated? Which don't appear correlated? Is there a variable that appears correlated but likely not informative? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots of the various features\n",
    "def calculate_correlation(x,y):\n",
    "    \"\"\"calculate pearson correlation\"\"\"\n",
    "    return calculate_mean((x - calculate_mean(x)).transpose() * (y - calculate_mean(y))) / np.sqrt(calculate_variance(x) * calculate_variance(y))\n",
    "    \n",
    "@widgets.interact(x=list(cancer_dataset), y=list(cancer_dataset))    \n",
    "def make_scatterplot(x=\"mean_radius\",y=\"mean_area\"):\n",
    "    \"\"\"make scatterplot with correlation value and regplot\"\"\"\n",
    "    colors=[\"#e28743\", \"#1e81b0\"]\n",
    "\n",
    "    corr = calculate_correlation(cancer_dataset[x], cancer_dataset[y])\n",
    "    index = int(corr > 0.5)\n",
    "    color = colors[index]\n",
    "    plt.title(r\"correlation: $\\rho = ${:.3f}\".format(corr), color= color, size=TITLE_FONT)\n",
    "    sns.scatterplot(data=cancer_dataset, x=x, y=y, alpha=0.5, color=color);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's split the variables up by their category (also called it's label in data science).\n",
    "\n",
    "Based on our available data, we're not that interested in what the descriptive statistics are on the individual columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_diagnoses_by_variable(variable: str, dataframe: pd.DataFrame = cancer_dataset):\n",
    "    \"\"\"Accepts column name to generate basic descriptions\"\"\"\n",
    "    df = dataframe.reset_index()\n",
    "    sns.boxplot(data=df, x=variable, y=df[\"diagnosis\"])\n",
    "    return dataframe[variable].groupby(\"diagnosis\").describe()\n",
    "\n",
    "@widgets.interact(variable=list(cancer_dataset))\n",
    "def comparison_wrapper(variable=\"mean_radius\"):\n",
    "    return compare_diagnoses_by_variable(variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run a two-sample t-test on these categories\n",
    "\n",
    "The t-test we learned in class focused on comparing our sample to a known quantity. We often don't know that quantity, but we can compare it to another sample mean as the estimate for that quantity. For the motivated, visit https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html to see the documentation to see how to write the code for a t-test.\n",
    "\n",
    "This test requires us to assume that the variances from the two samples share a population variance. Practically, if your standard deviations between samples is small (think less than a ratio of 3), you should be okay. In practice, this means that a typical t-test is inappropriate for comparing from two different populations, but a Welch t-test with a slightly different test statistic (that is still from a t-distribution) is used: https://en.wikipedia.org/wiki/Welch%27s_t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ind_ttest(feature=\"mean_radius\", dataset = cancer_dataset, welch=False):\n",
    "    \"\"\"run two sample t-tests\"\"\"\n",
    "    cat1 = dataset.xs(\"M\", level=1)\n",
    "    cat2 = dataset.xs(\"B\", level=1)\n",
    "    df = dataset.reset_index()\n",
    "    f, (ax_hist, ax_box) = plt.subplots(2, sharex=True)\n",
    "\n",
    "    sns.boxplot(data=df, x=feature, y=df[\"diagnosis\"], ax=ax_box)\n",
    "    sns.histplot(data=df, x=feature, hue=df[\"diagnosis\"], ax=ax_hist, stat=\"probability\", kde=True)\n",
    "    if welch: tstat, pvalue = ttest(cat1[feature], cat2[feature], equal_var=False)\n",
    "    else: tstat, pvalue = ttest(cat1[feature], cat2[feature])\n",
    "    print(f\"p-value: {pvalue:.2e}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use all the data points we have and compare malignant and benign tumors. Choose a variable and predict an outcome, you can then run a t-test to compare the means of the two groups. However, consider if the assumptions for a t-test are accurate (namely, do the variances come from the same sampling population)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(variable=list(cancer_dataset))\n",
    "def run_ttest(variable):\n",
    "    run_ind_ttest(feature=variable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Were there variables that you are uncomfortable making that assumption, based on the variance in the data? We can use a Welch's test that can account for that variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(variable=list(cancer_dataset))\n",
    "def run_welch_test(variable):\n",
    "    run_ind_ttest(feature=variable, welch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've compared the data based on all of the available data, but what if we were limited in the samples that we were able to collect? We will consider what happens to our comparisons and tests if we are only able to see a subsample of the available data. \n",
    "\n",
    "__Aside__: This process is similar to sampling from the empirical distribution function and is important for concepts like bootstrapping that you may encounter in further readings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@widgets.interact_manual(feature=list(cancer_dataset), n=(3,100))\n",
    "def run_sampled_ttest(feature=\"mean_radius\", n=3):\n",
    "    \"\"\"randomly sampled students t-test with same n\"\"\"\n",
    "    seed = 1\n",
    "    cat1 = cancer_dataset.xs(\"M\", level=1).sample(n, random_state=seed)\n",
    "    cat2 = cancer_dataset.xs(\"B\", level=1).sample(n)\n",
    "    \n",
    "    cat1[\"diagnosis\"] = \"M\"\n",
    "    cat2[\"diagnosis\"] = \"B\"\n",
    "    df = pd.concat([cat1,cat2]).reset_index()\n",
    "    f, (ax_hist, ax_box) = plt.subplots(2, sharex=True)\n",
    "\n",
    "    sns.boxplot(data=df, x=feature, y=df[\"diagnosis\"], ax=ax_box)\n",
    "    sns.histplot(data=df, x=feature, hue=df[\"diagnosis\"], ax=ax_hist, stat=\"probability\", kde=True)    \n",
    "    tstat, pvalue = ttest(cat1[feature], cat2[feature])\n",
    "    print(f\"p-value: {pvalue:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if we are under the impression that the data does not share the population variance, we can use the Welch t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact_manual(feature=list(cancer_dataset), n=(3,100))\n",
    "def run_sampled_welch_ttest(feature=\"mean_radius\", n=3):\n",
    "    \"\"\"randomly sampled welch t-test with same n\"\"\"\n",
    "    seed = 1\n",
    "    cat1 = cancer_dataset.xs(\"M\", level=1).sample(n, random_state=seed)\n",
    "    cat2 = cancer_dataset.xs(\"B\", level=1).sample(n)\n",
    "    \n",
    "    cat1[\"diagnosis\"] = \"M\"\n",
    "    cat2[\"diagnosis\"] = \"B\"\n",
    "    df = pd.concat([cat1,cat2]).reset_index()\n",
    "    f, (ax_hist, ax_box) = plt.subplots(2, sharex=True)\n",
    "\n",
    "    sns.boxplot(data=df, x=feature, y=df[\"diagnosis\"], ax=ax_box)\n",
    "    sns.histplot(data=df, x=feature, hue=df[\"diagnosis\"], ax=ax_hist, stat=\"probability\", kde=True)    \n",
    "    tstat, pvalue = ttest(cat1[feature], cat2[feature], equal_var=False)\n",
    "    print(f\"p-value: {pvalue:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the pdf of the _t_ distribution and the normal distribution\n",
    "\n",
    "The t-test is a largely robust method of comparing the mean to a value or the mean of another group. The method is generally robust for skewed distributions. Here we are going to compare the t-distribution to the normal distribution. Keeping in mind the CLT, what can we say about the point of diminishing returns with the $t$-distribution? Is there a cutoff point for degrees of freedom? What does this say about our norm (There is no simple answer here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t, norm\n",
    "\n",
    "@widgets.interact_manual(df=(3,100))\n",
    "def t_vs_norm(df=3):\n",
    "    \"\"\"plot the t-distribution vs the normal distribution pdfs\"\"\"\n",
    "    x = np.linspace(t.ppf(0.01, df), t.ppf(0.99, df), 100)\n",
    "    x = np.linspace(norm.ppf(0.01), norm.ppf(0.99), 100)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=FIG_SIZE)\n",
    "    ax.plot(x, t.pdf(x,df),'-', lw=5, alpha=0.6, label='t-distribution pdf')\n",
    "    ax.plot(x, norm.pdf(x),'k:', lw=5, alpha=0.6, label='normal distribution pdf')\n",
    "    plt.ylabel(\"Probability\", fontsize=LABEL_FONT)\n",
    "    plt.legend(loc=\"best\",fontsize=LABEL_FONT)\n",
    "    plt.xlabel(\"X\", fontsize=LABEL_FONT)\n",
    "    plt.title(r\"Student's $t$-Distribution vs. Normal Distribution\", fontsize=TITLE_FONT)\n",
    "    plt.ylim(0,.5)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Relationship between the _t_-distribution and the CLT \n",
    "\n",
    "If you are still confused what the origin of the _t_-distribution is, we will cover that here. \n",
    "This information will not be on an evaluation, but can provide you further intuition about how to think about $t$-tests. \n",
    "\n",
    "We are going to start from the CLT. \n",
    "We know that the sample means of a population are normally distributed, and therefore if we design our test-statistic with that in mind, we can some of the nice properties of the normal distribution. \n",
    "Although we simplified the CLT equation, it is more common to see it written as: \n",
    "\n",
    "\n",
    "$$\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow[]{d} N(0, 1)$$\n",
    "\n",
    "Meaning that the left hand side of the equation will equal the standard normal, or the normal distribution with a mean of 0 and a standard deviation of 1. \n",
    "\n",
    "Does the left hand side look familiar? Compare it to the table of test statistics. \n",
    "\n",
    "$$ t = \\frac{\\bar{X} - \\mu}{s_x/\\sqrt{n}} $$\n",
    "\n",
    "Because we generally don't know the $\\sigma$ of the underlying distribution (in general: we are trying to understand the population from a sample, not understand a sample from a population), we need to estimate $\\sigma$ with the sample standard deviation, $s_x$. \n",
    "Similarly to the CLT, we know that the the sample deviation has a chi-squared distribution, $\\chi^2$. \n",
    "This fact leads to the t statistic being the normal distribution, divided by the chi-squared distribution, yielding the $t$-distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
